{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "plt.style.use('default')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from utils.dataset import VideoDataset, MyConcatDataset, VideoDatasetRNN\n",
    "from utils.models import TrackNetV2MSE, TrackNetV2NLL, TrackNetV2RNN\n",
    "from utils.training import train_model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(image_size=(360, 640),\n",
    "                      sequence_length=4,\n",
    "                      sigma=5,\n",
    "                      drop_duplicate_frames=False,\n",
    "                      transform = ToTensor(),\n",
    "                      target_transform = ToTensor(),\n",
    "                      grayscale=False)\n",
    "\n",
    "dataset = VideoDatasetRNN(root=\"../datasets/prova/\", **dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "sequence_length = 4\n",
    "clear_probability = 0.9\n",
    "\n",
    "def collate_fn(batch):\n",
    "    frames, labels = default_collate(batch)\n",
    "\n",
    "    x = frames.clone()\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        if torch.rand(1) < clear_probability:\n",
    "            to_delete = torch.randint(low=1, high=sequence_length, size=(1,))\n",
    "            x[i, :to_delete] = torch.zeros(to_delete, x.shape[2], x.shape[3])\n",
    "    return x, torch.zeros(len(batch)), labels\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "input, a, labels = next(iter(dataloader))\n",
    "input = input.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input[0,3], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrackNetV2MSE(sequence_length=4)\n",
    "model.load('checkpoints/tracknet_v2_mse_360_640_4f/checkpoint_0020_best.ckpt')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrackNetV2RNN(sequence_length=4)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input[0].shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][i].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_hm = input[0]\n",
    "\n",
    "plt.imshow(input_hm[0][-1], cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(output[0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast(device_type='cpu'):\n",
    "    with torch.no_grad():\n",
    "        output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import OneCycleLR, StepLR\n",
    "from torch.optim import Adam\n",
    "from utils.models import TrackNetV2MSE\n",
    "\n",
    "model = TrackNetV2MSE()\n",
    "optimizer = Adam(model.parameters())\n",
    "scheduler = OneCycleLR(optimizer=optimizer, max_lr=1e-2, epochs=10, steps_per_epoch=100)\n",
    "\n",
    "scheduler.total_steps\n",
    "\n",
    "# TODO: Look for the scheduler for probabilities\n",
    "class MyScheduler(OneCycleLR):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def get_probability():\n",
    "        scheduler.total_steps\n",
    "\n",
    "s = MyScheduler(optimizer=optimizer, max_lr=1e-2, epochs=10, steps_per_epoch=100)\n",
    "s.last_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to find the bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "for i in range(1000):\n",
    "    dataset._generate_heatmap(np.random.randint(749, 780))\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "for i in range(1000):\n",
    "    dataset._get_heatmap(np.random.randint(749, 780))\n",
    "print(time.time()-t)\n",
    "\n",
    "t = time.time()\n",
    "for i in range(1000):\n",
    "    dataset_2._get_heatmap(np.random.randint(749, 780))\n",
    "print(time.time()-t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottleneck is in the ToTensor() Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "for i in range(100):\n",
    "    dataset[np.random.randint(0, 100)]\n",
    "print(time.time()-t)\n",
    "\n",
    "t = time.time()\n",
    "for i in range(10):\n",
    "    dataset_2[np.random.randint(0, 100)]\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "for i in range(100):\n",
    "    dataset[np.random.randint(0, 100)]\n",
    "print(time.time()-t)\n",
    "\n",
    "t = time.time()\n",
    "for i in range(10):\n",
    "    dataset_2[np.random.randint(0, 100)]\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "for i in range(100):\n",
    "    dataset[np.random.randint(0, 100)]\n",
    "print(time.time()-t)\n",
    "\n",
    "t = time.time()\n",
    "for i in range(10):\n",
    "    dataset_2[np.random.randint(0, 100)]\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, labels = dataset[0]\n",
    "frame1 = frames[0]\n",
    "\n",
    "frames, labels = dataset[10]\n",
    "frame2 = frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "for i in range(1000):\n",
    "    dataset._equal_frames(frame1, frame2)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize some activations and kernels because why not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrackNetV2MSE(sequence_length=3)\n",
    "model.load('checkpoints/tracknet_v2_mse_360_640/checkpoint_0027_best.ckpt')\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(image_size=(360, 640),\n",
    "                      sequence_length=3,\n",
    "                      sigma=5,\n",
    "                      drop_duplicate_frames=False,\n",
    "                      heatmap_mode='image',\n",
    "                      transform = ToTensor(),\n",
    "                      target_transform = ToTensor(),\n",
    "                      grayscale=False)\n",
    "\n",
    "dataset = VideoDataset(root=\"../datasets/prova/\", **dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "def get_encoding_layer(desired_block=1, subblock=0):\n",
    "    layers = []\n",
    "    for i, block in enumerate(model.children()):\n",
    "        # print(i)\n",
    "        if i%2 == 1:\n",
    "            layers.append(block)\n",
    "        for j, block_element in enumerate(block.children()):\n",
    "            #print(i, j)\n",
    "            for k, layer in enumerate(block_element.children()):\n",
    "                layers.append(layer)\n",
    "                # print(i, j, k)\n",
    "                if type(layer) is torch.nn.ReLU and i==2*desired_block and j==subblock:\n",
    "                    break\n",
    "            if type(layer) is torch.nn.ReLU and i==2*desired_block and j==subblock:\n",
    "                break\n",
    "        if type(layer) is torch.nn.ReLU and i==2*desired_block:\n",
    "            break\n",
    "    return layers\n",
    "\n",
    "def compute_activations(layers, input):\n",
    "    activation = input.unsqueeze(dim=0)\n",
    "    with torch.no_grad():\n",
    "        for l in layers:\n",
    "            activation = l(activation)\n",
    "\n",
    "    return activation.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, labels = dataset[50]\n",
    "frames = frames.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h, dpi = 300*2*16/9, 300, 100\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "axs[0].imshow(frames[-3:].numpy().transpose(1, 2, 0))\n",
    "axs[0].set_title(\"Input frame (last in sequence)\")\n",
    "\n",
    "axs[1].imshow(labels[0])\n",
    "axs[1].set_title(\"Ground truth\")\n",
    "\n",
    "fig.tight_layout(pad=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_part = np.linspace(0, 1, 10)\n",
    "c = []\n",
    "\n",
    "for n in noise_part:\n",
    "    with torch.no_grad():\n",
    "        f = (1-n)*frames + n*torch.randn(frames.shape)\n",
    "        out = model(f.unsqueeze(dim=0)).squeeze().numpy()\n",
    "    c.append(out.max())\n",
    "plt.plot(noise_part, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0.07\n",
    "with torch.no_grad():\n",
    "    f = (1-n)*frames + n*torch.randn(frames.shape)\n",
    "    out = model(f.unsqueeze(dim=0)).squeeze().numpy()\n",
    "plt.imshow(out)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = 2\n",
    "subblock = 2\n",
    "\n",
    "activations = compute_activations(get_encoding_layer(block, subblock), frames)\n",
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dead_activations, ) = np.where(activations.max(axis=(1,2))==0)\n",
    "print(f\"Of {activations.shape[0]} activations, {dead_activations.size} are dead and {activations.shape[0]-dead_activations.size} are not.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_pixels = 1080\n",
    "top_adjust = 1\n",
    "\n",
    "w, h, dpi = height_pixels*16/9*top_adjust, height_pixels, 100\n",
    "fig, axs = plt.subplots(nrows=8, ncols=8, figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "i_0 = 0\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(activations[i+i_0], cmap='gray')\n",
    "    # ax.set_title(i)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "#fig.suptitle(f\"Activations in encoding block {block}, subblock {subblock}\")\n",
    "\n",
    "fig.tight_layout(pad=0.5)\n",
    "fig.subplots_adjust(top=top_adjust)\n",
    "\n",
    "fig.savefig(f\"{block}_{subblock}.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "\n",
    "kernels = model.state_dict()['vgg_conv1.1.0.weight'].numpy()\n",
    "biases = model.state_dict()['vgg_conv1.1.0.bias'].numpy()\n",
    "w, h, dpi = 800, 800, 100\n",
    "fig, axs = plt.subplots(nrows=8, ncols=8, figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "print(kernels.shape)\n",
    "print(biases[k])\n",
    "\n",
    "min_val = kernels[k].min()\n",
    "max_val = kernels[k].max()\n",
    "print(min_val, max_val)\n",
    "\n",
    "max_val=max((max_val, -min_val))\n",
    "min_val=min((-max_val, min_val))\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(kernels[k,i], cmap='RdBu', vmin=min_val, vmax=max_val)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "#fig.suptitle(f\"Kernel {k}, bias = {biases[k]:.2g}\")\n",
    "fig.tight_layout(pad=0.2)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fc0ed74a087944f7d4394f4d91ee1483def030b33d45c86fb544dde79387957"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
