{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from detection.testing import _get_checkpoint_filename, _get_results_folder\n",
    "from detection.visualization import show_loss_history\n",
    "\n",
    "from train_configurations.utils import get_standard_test_dataset\n",
    "from train_configurations import (regnet_y_400mf, regnet_y_800mf,\n",
    "                                  tracknet_v2, tracknet_v2_2f,\n",
    "                                  tracknet_v2_4f, tracknet_v2_6f,\n",
    "                                  tracknet_v2_rnn, tracknet_v2_rnn_scheduler)\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "detection_folder = '../TFM/figures/detection'\n",
    "trajectories_folder = '../TFM/figures/trajectories'\n",
    "\n",
    "if not os.path.exists(detection_folder):\n",
    "    os.makedirs(detection_folder)\n",
    "if not os.path.exists(trajectories_folder):\n",
    "    os.makedirs(trajectories_folder)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# sequence of train configurations\n",
    "training_configurations = [regnet_y_400mf, regnet_y_800mf,\n",
    "                           tracknet_v2_2f, tracknet_v2,\n",
    "                           tracknet_v2_4f, tracknet_v2_6f,\n",
    "                           tracknet_v2_rnn, tracknet_v2_rnn_scheduler]\n",
    "\n",
    "\n",
    "# get the name of the train configuration as string\n",
    "def tc_name(training_configuration):\n",
    "    \"\"\"Return the name of the train configuration as string\"\"\"\n",
    "    name = training_configuration.__name__.split('.')[-1]\n",
    "    if name == 'tracknet_v2':\n",
    "        name = 'tracknet_v2_3f'\n",
    "    if name == 'tracknet_v2_rnn_scheduler':\n",
    "        name = 'tracknet_v2_rnn_s'\n",
    "    return name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures and train configuration explanations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table for the architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "n = [3, 3, 2, 3, 4, 6, 1, 1]\n",
    "h = [0, 0, 0, 0, 0, 0, 3, 3]\n",
    "epochs = [40, 40, 20, 42, 23, 20, 35, 35]\n",
    "batch_size = [16, 16, 8, 8, 8, 8, 8, 8]\n",
    "for ti, tc in enumerate(training_configurations):\n",
    "    d = {'configuration': tc_name(tc),\n",
    "         'n': n[ti],\n",
    "         'h': h[ti],\n",
    "         'epochs': epochs[ti],\n",
    "         'batch size': batch_size[ti]\n",
    "         }\n",
    "    table.append(d)\n",
    "\n",
    "output_df = pd.DataFrame(table)\n",
    "display(output_df)\n",
    "print(output_df.to_latex(index=False))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's define some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_model(training_configuration, training_phase=None):\n",
    "    \"\"\"Get the model of the provided train configuration with the final weights.\"\"\"\n",
    "    config = training_configuration.Config()\n",
    "\n",
    "    checkpoint_folder = config._checkpoint_folder\n",
    "    if training_phase is not None:\n",
    "        checkpoint_folder = os.path.join(checkpoint_folder, training_phase)\n",
    "\n",
    "    results_folder = _get_results_folder(checkpoint_folder, None)\n",
    "    checkpoint_path = os.path.join(results_folder, _get_checkpoint_filename(checkpoint_folder))\n",
    "\n",
    "    model = config.get_model()\n",
    "    model.eval()\n",
    "    model.load(checkpoint_path, device='cpu')\n",
    "\n",
    "    return get_standard_test_dataset(training_configuration, 'prova', is_rnn=training_phase is not None), model\n",
    "\n",
    "\n",
    "def get_output(model, dataset, i):\n",
    "    \"\"\"Get the model output for the given dataset element.\"\"\"\n",
    "    frame, _ = dataset[i]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(frame.to(torch.float32).unsqueeze(0)).numpy().squeeze()\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_frame(dataset, i):\n",
    "    \"\"\"Get the last input frame of the given dataset element element.\"\"\"\n",
    "    frame, _ = dataset[i]\n",
    "    return frame[-3:].to(torch.float32).numpy().transpose(1, 2, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize an example of input frames and target heatmap. In this example, we will use the ``tracknet_v2_2f`` training configuration.\n",
    "\n",
    "In addition, the model output is also shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from the desired train configuration\n",
    "dataset, model = get_trained_model(tracknet_v2_2f)\n",
    "\n",
    "dataset_element = 15\n",
    "frames, heatmap = dataset[dataset_element]\n",
    "\n",
    "frames = frames.to(torch.float32)\n",
    "frames = frames.numpy().transpose(1, 2, 0)\n",
    "frames = [frames[:,:,3*i:3*(i+1)] for i in range(2)]\n",
    "\n",
    "heatmap = heatmap.to(torch.float32).squeeze().numpy()\n",
    "\n",
    "heatmap_pred = get_output(model, dataset, dataset_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h, dpi = 2*640, 2*360, 100\n",
    "fig, axs = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi, nrows=2, ncols=2)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for ti, (frame, ax) in enumerate(zip(frames, axs)):\n",
    "    ax.set_title(f'Frame {ti+1}')\n",
    "    ax.imshow(frame)\n",
    "\n",
    "    x = dataset._label_df['x'][ti+dataset_element]\n",
    "    y = dataset._label_df['y'][ti+dataset_element]\n",
    "    ax.scatter(x*frame.shape[1], y*frame.shape[0], zorder=100, facecolors='none', edgecolors='y', linewidths=3, s=150)\n",
    "\n",
    "axs[2].imshow(frames[-1])\n",
    "axs[2].imshow(heatmap, alpha=0.5, cmap='gray', vmin=0, vmax=1)\n",
    "axs[2].set_title('Target heatmap')\n",
    "\n",
    "axs[3].imshow(frames[-1])\n",
    "axs[3].imshow(heatmap_pred, alpha=0.5, cmap='gray', vmin=0, vmax=1)\n",
    "axs[3].set_title('Predicted heatmap')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(os.path.join(detection_folder, 'sample_input_a.png'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h, dpi = 640, 360, 100\n",
    "fig, ax = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "ax.imshow(frames[-1])\n",
    "\n",
    "ax.set_axis_off()\n",
    "fig.tight_layout(pad=0)\n",
    "fig.savefig(os.path.join(detection_folder, 'sample_input_frame.png'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "w, h, dpi = 640, 360, 100\n",
    "fig, ax = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "ax.imshow(frames[-1])\n",
    "ax.imshow(heatmap_pred, alpha=0.5, cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "ax.set_axis_off()\n",
    "fig.tight_layout(pad=0)\n",
    "fig.savefig(os.path.join(detection_folder, 'sample_input_overlay.png'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "w, h, dpi = 640, 360, 100\n",
    "fig, ax = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "ax.imshow(heatmap_pred, cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "ax.set_axis_off()\n",
    "fig.tight_layout(pad=0)\n",
    "fig.savefig(os.path.join(detection_folder, 'sample_input_heatmap.png'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for all models\n",
    "\n",
    "Here we visualize the output of all the various models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the output heatmap for the various tracknet variants superimposed on the last frame of the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tc in training_configurations[2:]:\n",
    "    print(tc_name(tc))\n",
    "    sample = 100\n",
    "    if 'rnn' in tc_name(tc):\n",
    "        training_phase = 'phase_3_0'\n",
    "    else:\n",
    "        training_phase = None\n",
    "        sl = tc.Config()._sequence_length\n",
    "        sample -= sl\n",
    "\n",
    "    dataset, model = get_trained_model(tc, training_phase)\n",
    "    frame = get_frame(dataset, sample)\n",
    "    output =  get_output(model, dataset, sample)\n",
    "\n",
    "    w, h, dpi = 640, 360, 120\n",
    "    fig, ax = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "    ax.imshow(frame)\n",
    "    ax.imshow(output, alpha=0.5, cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    fig.tight_layout(pad=0)\n",
    "\n",
    "    fig.savefig(os.path.join(detection_folder, f\"{tc_name(tc)}_example.png\"))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regnet and Standard configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection.visualization import get_loss_history\n",
    "get_loss_history(tc)[0].min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tc in enumerate(training_configurations[:-2]):\n",
    "    w, h, dpi = 640, 420, 130\n",
    "    fig, ax = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    epoch_range = (1, 20) if i>=2 else None\n",
    "    ax = show_loss_history(tc, ax=ax, epoch_range=epoch_range)\n",
    "\n",
    "    if epoch_range is not None:\n",
    "        ax.set_xticks(np.arange(0, 21, 5))\n",
    "    ax.set_ylim(2e-5, 5e-2)\n",
    "\n",
    "    ax.set_title(tc_name(tc))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(detection_folder, f'{tc_name(tc)}_loss.pdf'))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tc in training_configurations[-2:]:\n",
    "    phases = np.array([0, 10, 15, 20, 25, 30, 35]) + 0.5\n",
    "\n",
    "    w, h, dpi = 640, 420, 130\n",
    "    fig, ax = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "    ax = show_loss_history(tc, ax=ax, epoch_range=(1, 35))\n",
    "\n",
    "    xlim = ax.get_xbound()\n",
    "    ylim = ax.get_ybound()\n",
    "\n",
    "    ax.set_ylim(2e-5, 5e-2)\n",
    "    ylim = ax.get_ybound()\n",
    "\n",
    "    for ti, p in enumerate(phases[1:-1]):\n",
    "        ax.plot([p,p], [ylim[0], ylim[1]], 'k--', linewidth=0.5 if ti%2==0 else 0.7)\n",
    "\n",
    "    # ax.set_ylim(ylim)\n",
    "\n",
    "    bbox = {'boxstyle': 'round',\n",
    "            'facecolor': 'w',\n",
    "            'edgecolor': 'None',\n",
    "            'alpha': 1}\n",
    "\n",
    "    for ti, c in enumerate(['r', 'g', 'b']):\n",
    "        t = 5e-2\n",
    "        xpos = 10*ti+6 if ti==0 else 10*ti+8\n",
    "        ax.annotate(f'Phase {ti+1}', [xpos, t*ylim[1]+(1-t)*ylim[0]], bbox=bbox)\n",
    "        ax.fill_between([phases[2*ti], phases[2*(ti+1)]], ylim[0], ylim[1], color=c, alpha=0.15)\n",
    "\n",
    "    ax.legend(loc='upper right', framealpha=1)\n",
    "    ax.set_title(tc_name(tc))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(detection_folder, f'{tc_name(tc)}_loss.pdf'))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results\n",
    "\n",
    "Compute the position error from the csv outputted from model testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the optimal detection threshold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the utility function ``get_output_df`` to get a dataframe from the csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_df(training_configuration, training_phase=None):\n",
    "    config = training_configuration.Config()\n",
    "\n",
    "    checkpoint_folder = config._checkpoint_folder\n",
    "    if training_phase is not None:\n",
    "        checkpoint_folder = os.path.join(checkpoint_folder, training_phase)\n",
    "    try:\n",
    "        results_folder = _get_results_folder(checkpoint_folder, None)\n",
    "        df = pd.read_csv(os.path.join(results_folder, 'output_val.csv'))\n",
    "        return df.copy().loc[(df['x_true']!=0) & (df['y_true']!=0)] # extract only the labelled frames\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        if training_phase is None:\n",
    "            return get_output_df(training_configuration, training_phase='phase_3_0')\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Output not found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions to find the error distribution, and the mean error as function of the detection threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_distribution(output_df, threshold=None, image_size=(360, 640)):\n",
    "    if threshold is not None:\n",
    "        df = output_df.loc[output_df['max_values']>=threshold]\n",
    "    else:\n",
    "        df = output_df\n",
    "    x_true, x_pred, y_true, y_pred = [df[k].values for k in ['x_true', 'x_pred', 'y_true', 'y_pred']]\n",
    "    return np.sqrt((image_size[1] * (x_true-x_pred))**2 + (image_size[0] * (y_true-y_pred))**2)\n",
    "\n",
    "\n",
    "def detection_error_curve(output_df, num_thresholds=501):\n",
    "    thresholds = np.linspace(0, 1, num_thresholds)\n",
    "\n",
    "    # compute the detection rate for each threhsold\n",
    "    detection_rate = [len(output_df.loc[output_df['max_values']>=t]) / len(output_df) for t in thresholds]\n",
    "\n",
    "    # compute the mean positioning error\n",
    "    mean_errors = []\n",
    "    median_errors = []\n",
    "    std_errors = []\n",
    "    # precision = []\n",
    "    for t in thresholds:\n",
    "        error_distribution = compute_error_distribution(output_df, t)\n",
    "        if len(error_distribution) == 0:\n",
    "            mean_errors.append(0)\n",
    "            continue\n",
    "        mean_errors.append(np.mean(error_distribution))\n",
    "        median_errors.append(np.median(error_distribution))\n",
    "        std_errors.append(np.std(error_distribution))\n",
    "        # precision.append(np.flatnonzero(error_distribution <= distance_threshold).size/error_distribution.size)\n",
    "\n",
    "    return thresholds, np.asarray(detection_rate), np.asarray(mean_errors), np.asarray(median_errors), np.asarray(std_errors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection and positioning error as function of threshold (all in one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_error = []\n",
    "median_error = []\n",
    "std_error = []\n",
    "dr = []\n",
    "\n",
    "w, h, dpi = 1280, 720, 120\n",
    "fig, axs = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi, nrows=2, ncols=3)\n",
    "\n",
    "for i, (tc, ax) in enumerate(zip(training_configurations[2:], axs.T.ravel())):\n",
    "    threhsolds, detection_rates, mean_errors, median_errors, std_errors = detection_error_curve(get_output_df(tc), 501)\n",
    "\n",
    "    w = np.where(mean_errors > 0)\n",
    "    threhsolds = threhsolds[w]\n",
    "    detection_rates = detection_rates[w]\n",
    "    mean_errors = mean_errors[w]\n",
    "\n",
    "    # set chosen threshold\n",
    "    t = 0.1\n",
    "    ti = np.argmin(np.abs(t - threhsolds))\n",
    "\n",
    "    # positioning error\n",
    "    ax.plot([t,t], [0,max(mean_errors)], '--', color='#000000', linewidth=1, label=f'chosen $v_{{th}}$ = {t}')\n",
    "    ax.plot(threhsolds, mean_errors, 'C0-', label= f'Positioning error = {mean_errors[ti]:.3g} px')\n",
    "    ax.plot([0,1], [mean_errors[ti], mean_errors[ti]], '-', color='C0', linewidth=1)\n",
    "\n",
    "    # detection rate\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(threhsolds, detection_rates, 'C1-', label=f'Detection rate = {100*detection_rates[ti]:.0f}%')\n",
    "    ax2.plot([0,1], [detection_rates[ti], detection_rates[ti]], '-', color='C1', linewidth=1)\n",
    "\n",
    "    # axis limits and log scale on positioning error\n",
    "    ax.set_xlim(0, 1)\n",
    "    # ax.set_yscale('log')\n",
    "    # ax.set_ylim(0.5, 38)\n",
    "    ax.set_ylim(0,max(mean_errors))\n",
    "\n",
    "    # axis limits on detection rate\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    # set title\n",
    "    ax.set_title(tc_name(tc))\n",
    "\n",
    "    # set x-abels\n",
    "    if i%2==1:\n",
    "        ax.set_xlabel('$v_{th}$')\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "    # set y-labels\n",
    "    if i<2:\n",
    "        ax.set_ylabel('Positioning error')\n",
    "    # else:\n",
    "    #     ax.set_yticklabels([])\n",
    "\n",
    "    if i>=4:\n",
    "        ax2.set_ylabel('Detection rate')\n",
    "    else:\n",
    "        ax2.set_yticklabels([])\n",
    "\n",
    "    # show legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(handles + handles2, labels + labels2, loc='center', fontsize='small')\n",
    "\n",
    "    mean_error.append(mean_errors[ti])\n",
    "    median_error.append(median_errors[ti])\n",
    "    std_error.append(std_errors[ti])\n",
    "    dr.append(detection_rates[ti])\n",
    "\n",
    "fig.suptitle(\"Detection rate and positioning error as a funciton of the detection threshold\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(detection_folder, f'threhsold_curves.pdf'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean error as function of the detection rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = training_configurations[2]\n",
    "\n",
    "mean_error = []\n",
    "median_error = []\n",
    "std_error = []\n",
    "dr = []\n",
    "\n",
    "for tc in training_configurations[2:]:\n",
    "    # w, h, dpi = 640*2, 360, 100\n",
    "    w, h, dpi = 1280, 480, 120\n",
    "    fig, axs = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi, ncols=2)\n",
    "\n",
    "    threhsolds, detection_rates, mean_errors, median_errors, std_errors = detection_error_curve(get_output_df(tc), 501)\n",
    "\n",
    "    w = np.where(mean_errors > 0)\n",
    "    threhsolds = threhsolds[w]\n",
    "    detection_rates = detection_rates[w]\n",
    "    mean_errors = mean_errors[w]\n",
    "\n",
    "    t = 0.1\n",
    "\n",
    "    ti = np.argmin(np.abs(t - threhsolds))\n",
    "\n",
    "    # plot detection rate as function of threshold\n",
    "    axs[0].plot(threhsolds, detection_rates, 'C0-', label=f'detection rate = {100*detection_rates[ti]:.0f}%')\n",
    "    axs[0].plot([0,1], [detection_rates[ti], detection_rates[ti]], '-', color='C0', linewidth=1)\n",
    "    axs[0].set_xlabel('Threhsold')\n",
    "    axs[0].set_ylabel('Detection rate')\n",
    "    axs[0].set_ylim(0, 1)\n",
    "\n",
    "    ax2 = axs[0].twinx()\n",
    "\n",
    "    # plot positioning error as function of threshold\n",
    "    ax2.plot([t,t], [0,max(mean_errors)], '--', color='#000000', linewidth=1, label=f'chosen threshold = {t}')\n",
    "    ax2.plot(threhsolds, mean_errors, 'C1-', label= f'$\\Delta$ p = {mean_errors[ti]:.3g} px')\n",
    "    ax2.plot([0,1], [mean_errors[ti], mean_errors[ti]], '-', color='C1', linewidth=1)\n",
    "    ax2.set_xlabel('Threshold')\n",
    "    ax2.set_ylabel('$\\Delta$ p')\n",
    "    ax2.set_ylim(0, max(mean_errors))\n",
    "\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(handles2 + handles, labels2 + labels, loc='center')\n",
    "\n",
    "\n",
    "    auc = -np.trapz(mean_errors, x=detection_rates)\n",
    "\n",
    "    axs[1].plot(detection_rates, mean_errors, color='#000000', label=f'auc = {auc:.3g}')\n",
    "    axs[1].scatter(detection_rates[ti], mean_errors[ti], c='#000000', s=50)\n",
    "    axs[1].set_xlabel('Detection rate')\n",
    "    axs[1].set_ylabel('Mean error')\n",
    "    axs[1].set_ylim(0, max(mean_errors))\n",
    "\n",
    "    axs[1].legend()\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlim(0, 1.02)\n",
    "        ax.set_ylim(0, 12)\n",
    "\n",
    "    fig.suptitle(tc_name(tc))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(detection_folder, f'{tc_name(tc)}.pdf'))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    mean_error.append(mean_errors[ti])\n",
    "    median_error.append(median_errors[ti])\n",
    "    std_error.append(std_errors[ti])\n",
    "    dr.append(detection_rates[ti])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = training_configurations[2]\n",
    "\n",
    "w, h, dpi = 1280, 720, 150\n",
    "fig, ax = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "ym = 0\n",
    "\n",
    "for i, tc in enumerate(training_configurations[2:5]):\n",
    "    threhsolds, detection_rates, mean_errors, median_errors, std_errors = detection_error_curve(get_output_df(tc), 1001)\n",
    "\n",
    "    # get only those elements where the mean error is greater \n",
    "    w = np.where(mean_errors > 0)\n",
    "    threhsolds = threhsolds[w]\n",
    "    detection_rates = detection_rates[w]\n",
    "    mean_errors = mean_errors[w]\n",
    "\n",
    "    # t = 0.1\n",
    "    # ti = np.argmin(np.abs(t - threhsolds))\n",
    "    # em = 1.5\n",
    "    # ti = np.argmin(np.abs(mean_errors - em))\n",
    "    dr = 0.97\n",
    "    ti = np.argmin(np.abs(detection_rates - dr))\n",
    "\n",
    "    # plot error-detection curve\n",
    "    # auc = -np.trapz(mean_errors, x=detection_rates)\n",
    "    # ax.plot(detection_rates, mean_errors, label=f'{tc_name(tc)}:\\n     auc = {auc:.3g}', color=f'C{i}')\n",
    "    ax.plot(detection_rates, mean_errors, label=f'{tc_name(tc)}:\\n  dr = {detection_rates[ti]:.3g}\\n  t = {threhsolds[ti]:.3g}\\n  e = {mean_errors[ti]:.3g}', color=f'C{i}')\n",
    "    # ax.scatter(detection_rates[ti], mean_errors[ti], s=50, c=f'C{i}')\n",
    "\n",
    "    # show correspondence lines at threshold 0.1\n",
    "    ax.plot([detection_rates[ti], detection_rates[ti]], [0,100], '--', color=f'C{i}', linewidth=1)\n",
    "    ax.plot([0,2], [mean_errors[ti], mean_errors[ti]], '--', color=f'C{i}', linewidth=1)\n",
    "\n",
    "    ym = max(ym, max(mean_errors))\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel('Detection rate')\n",
    "ax.set_ylabel('Mean error')\n",
    "\n",
    "ax.set_xlim(0.8, 1)\n",
    "ax.set_ylim(0, 3)\n",
    "\n",
    "fig.suptitle(\"Mean error as function of the detection rate\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(detection_folder, f'error_detection_curve.pdf'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dataframe for each train configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = []\n",
    "for tc in training_configurations:\n",
    "    # phase = 'phase_3_0' if 'rnn' in tc_name(tc) else None\n",
    "    df = get_output_df(tc)\n",
    "    if 'reg' in tc_name(tc):\n",
    "        df_out.append(df)\n",
    "    else:\n",
    "        df2 = df.copy().loc[df['max_values'] > 0.1]\n",
    "        df2['detection_rate'] = np.zeros(len(df2)) + len(df2)/len(df)\n",
    "        df_out.append(df2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the position error as the magnitude of the difference between the true position and the estimated position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (360, 640)\n",
    "\n",
    "for output_df in df_out:\n",
    "    bbb=compute_error_distribution(output_df)\n",
    "    output_df['error']=compute_error_distribution(output_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error histograms\n",
    "\n",
    "Here we visualize the error distributions for the various models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary dataframe with most important statistics of the error:\n",
    " - mean\n",
    " - standard deviation\n",
    " - median\n",
    " - percentage with error < 1 px\n",
    " - percentage with error < 5 px\n",
    " - percentage with error < 10 px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {}\n",
    "r['a'] = 9\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [1, 5, 10] #error strictly smaller than the threshold\n",
    "\n",
    "summary = []\n",
    "for tc, df in zip(training_configurations, df_out):\n",
    "    result = {}\n",
    "    result['train_configuration'] = tc_name(tc)\n",
    "\n",
    "    if 'tracknet' in tc_name(tc):\n",
    "        result['detection rate'] = np.mean(df['detection_rate'])\n",
    "    else:\n",
    "        result['detection rate'] = 1\n",
    "\n",
    "    s = df['error'].size\n",
    "    if s==0:\n",
    "        result['mean'] = np.nan\n",
    "        result['std'] = np.nan\n",
    "        # result['median'] = np.nan\n",
    "    else:\n",
    "        result['mean'] = np.mean(df['error'])\n",
    "        result['std'] = np.std(df['error'])\n",
    "        # result['median'] = np.median(df['error'])\n",
    "\n",
    "    for t in thresholds:\n",
    "        if s==0:\n",
    "            result[f'{t} px'] = np.nan\n",
    "        else:\n",
    "            result[f'{t} px'] = np.flatnonzero(np.asarray(df['error']) < t).size / s\n",
    "    summary.append(result)\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the mean, median and std of the errors as {:.3g}\n",
    "# get the error rate as a percentage and format that as {:.2f}\n",
    "df_s2 = df_summary.copy()\n",
    "\n",
    "for k in df_s2.columns:\n",
    "    if 'px' in k or 'detection' in k:\n",
    "        df_s2[k] = df_s2[k].apply(lambda x: f'{100*x:.3g}%')\n",
    "    else:\n",
    "        df_s2[k] = df_s2[k].apply(lambda x: f'{x:.3g}' if type(x) is float else x)\n",
    "\n",
    "print(df_s2.to_latex(index=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize error distribution histograms (one ifigure per configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(summary: dict):\n",
    "    for k, v in summary.items():\n",
    "        print_v = v\n",
    "        if 'px' in k:\n",
    "            print_v = f'{100*v:.2g}%'\n",
    "        elif k!='train_configuration':\n",
    "            print_v = f'{v:.2g}'\n",
    "        print(f\"{k}: {print_v}\")\n",
    "\n",
    "\n",
    "hist_range = thresholds[-1]\n",
    "\n",
    "for tc, df, s in zip(training_configurations, df_out, summary):\n",
    "    print_summary(s)\n",
    "\n",
    "    w, h, dpi = 640, 480, 120\n",
    "    fig, ax = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "    ax.hist(df['error'].clip(upper=hist_range), bins=np.arange(hist_range+2), density=True)#, align='left', rwidth=0.8)\n",
    "\n",
    "    # legend\n",
    "    for k in ['mean', 'std']:\n",
    "        ax.plot([], [], ' ', label=f'{k}:'.ljust(14) + f'{s[k]:.1f}'.rjust(5) + ' ')\n",
    "    for k in [k for k in s.keys() if 'px' in k]:\n",
    "        ax.plot([], [], ' ', label='error < ' + f'{k}:'.rjust(6) + f'{100*s[k]:.1f}%'.rjust(6) + ' ')\n",
    "\n",
    "    ax.legend(handlelength=0, prop={'family': 'monospace'}, loc='upper center')\n",
    "\n",
    "    # set x ticks\n",
    "    ax.set_xticks(np.arange(0, 11, 2))\n",
    "\n",
    "    # set axis limits\n",
    "    ax.set_xlim(-0.5, 11.5)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    # set title and axis labels\n",
    "    ax.set_title(tc_name(tc))\n",
    "    ax.set_xlabel('Error magnitude')\n",
    "    ax.set_ylabel('Occurrence')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(detection_folder, f'{tc_name(tc)}_error.pdf'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize error distribution histograms (all in one figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_range = thresholds[-1]\n",
    "\n",
    "w, h, dpi = 1200, 600, 110\n",
    "fig, axs = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi, nrows=2, ncols=4)\n",
    "\n",
    "axs = axs.T.ravel()\n",
    "\n",
    "for ti, (tc, df, s, ax) in enumerate(zip(training_configurations, df_out, summary, axs)):\n",
    "    ax.hist(df['error'].clip(upper=hist_range), bins=np.arange(hist_range+2), density=True)#, align='left', rwidth=0.8)\n",
    "\n",
    "    # legend\n",
    "    for k in ['mean', 'std']:\n",
    "        ax.plot([], [], ' ', label=f'{k}:'.ljust(14) + f'{s[k]:.1f}'.rjust(5) + ' ')\n",
    "    for k in [k for k in s.keys() if 'px' in k]:\n",
    "        ax.plot([], [], ' ', label='error < ' + f'{k}:'.rjust(6) + f'{100*s[k]:.1f}%'.rjust(6) + ' ')\n",
    "\n",
    "    ax.legend(handlelength=0, prop={'family': 'monospace', 'size': 'small'}, loc='upper center')\n",
    "\n",
    "    # set title and axis labels\n",
    "    ax.set_title(tc_name(tc))\n",
    "    if ti<2:\n",
    "        ax.set_ylabel('Occurrence')\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    ax.set_xticks(np.arange(0, 11, 2))\n",
    "    if ti%2==1:\n",
    "        ax.set_xlabel('Positioning error in px')\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "    # set axis limits\n",
    "    ax.set_xlim(-0.5, 11.5)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(detection_folder, f'errors.pdf'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error as function of maximum heatmap value\n",
    "\n",
    "Here we do a scatterplot of the error and the maximum heatmap value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def linear(x, a, b):\n",
    "    return a*x + b\n",
    "\n",
    "def quadratic(x, a, b, c):\n",
    "    return a*x*x + b*x + c\n",
    "\n",
    "for tc, df in zip(training_configurations[2:], df_out[2:]):\n",
    "    w, h, dpi = 640, 360, 120\n",
    "    fig, ax = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "    # sns.histplot(x=df['max_values'], y=df['error'], bins=(np.linspace(0, 1, 50), np.arange(15)), ax=ax, cbar=True)\n",
    "    ax.scatter(x=df['max_values'], y=df['error'], alpha=0.5)\n",
    "\n",
    "    idx = df['max_values'] > 0.2\n",
    "    if idx.values.sum() > 0:\n",
    "        params, cov = curve_fit(linear, df['max_values'][idx], df['error'][idx])\n",
    "        x = np.linspace(0, 1, 2)\n",
    "        plt.plot(x, linear(x, *params), 'k-')\n",
    "\n",
    "        params, cov = curve_fit(quadratic, df['max_values'][idx], df['error'][idx])\n",
    "        x = np.linspace(0, 1, 101)\n",
    "        plt.plot(x, quadratic(x, *params), 'r-')\n",
    "\n",
    "    ax.set_xlabel('max heatmap value')\n",
    "    ax.set_ylabel('position error')\n",
    "\n",
    "    ax.set_xlim(-0.05, 1.05)\n",
    "    ax.set_title(tc_name(tc))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectories fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trajectories.data_reading import get_candidates, get_frame, get_heatmap\n",
    "\n",
    "from trajectories.fitting import fit_trajectories\n",
    "from trajectories.filtering import build_trajectory_graph, find_shortest_paths, build_path_mapping\n",
    "from trajectories.visualization import create_trajectory_video, visualize_trajectory_graph, show_neighboring_trajectories\n",
    "\n",
    "starting_frame, candidates, n_candidates, values = get_candidates(tracknet_v2)\n",
    "frame_sequence = list(range(starting_frame, starting_frame + len(candidates)))\n",
    "\n",
    "fitting_info = fit_trajectories(candidates, n_candidates, starting_frame)\n",
    "trajectory_graph = build_trajectory_graph(fitting_info)\n",
    "shortest_paths = find_shortest_paths(trajectory_graph)\n",
    "path_mapping = build_path_mapping(fitting_info, shortest_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory graph example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = visualize_trajectory_graph(trajectory_graph, shortest_paths[0][0], 889)\n",
    "fig = ax.figure\n",
    "fig.savefig(os.path.join(trajectories_folder, f'graph_example.pdf'))\n",
    "plt.close(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "dpi = 150\n",
    "dark_mode = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big examples for success and failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# success example\n",
    "\n",
    "sf = 996\n",
    "num_prev = 4\n",
    "num_next = 3\n",
    "filename = f'example.png'\n",
    "fig, ax = create_trajectory_video(tracknet_v2, os.path.join(trajectories_folder, filename),\n",
    "                                  fitting_info=fitting_info, path_mapping=path_mapping,\n",
    "                                  starting_frame=sf,\n",
    "                                  dark_mode=dark_mode, dpi=dpi,\n",
    "                                  num_prev=num_prev, num_next=num_next,\n",
    "                                  num_frames=0)\n",
    "plt.close(fig)\n",
    "\n",
    "# fail example\n",
    "sf = 1502\n",
    "num_prev=2\n",
    "num_next=3\n",
    "\n",
    "filename = f'example_fail.png'\n",
    "fig, ax = create_trajectory_video(tracknet_v2, os.path.join(trajectories_folder, filename),\n",
    "                                  fitting_info=fitting_info, path_mapping=path_mapping,\n",
    "                                  starting_frame=sf,\n",
    "                                  dark_mode=dark_mode, dpi=dpi,\n",
    "                                  num_prev=num_prev, num_next=num_next,\n",
    "                                  num_frames=0)\n",
    "plt.close(fig)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shot example (near team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sf in [983, 991, 992, 996]:\n",
    "    if sf<=991:\n",
    "        num_prev = 3\n",
    "        num_next = 4\n",
    "    else:\n",
    "        num_prev = 4\n",
    "        num_next = 3\n",
    "    filename = f'{sf}.png'\n",
    "    fig, ax = create_trajectory_video(tracknet_v2, os.path.join(trajectories_folder, filename),\n",
    "                                      fitting_info=fitting_info, path_mapping=path_mapping,\n",
    "                                      starting_frame=sf,\n",
    "                                      dark_mode=dark_mode, dpi=dpi,\n",
    "                                      num_prev=num_prev, num_next=num_next,\n",
    "                                      num_frames=0)\n",
    "    plt.close(fig)\n",
    "    clear_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounce example (floor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sf in [1006, 1007, 1016]:\n",
    "    if sf<=1006:\n",
    "        num_prev = 4\n",
    "        num_next = 3\n",
    "    else:\n",
    "        num_prev = 5\n",
    "        num_next = 2\n",
    "    filename = f'{sf}.png'\n",
    "    fig, ax = create_trajectory_video(tracknet_v2, os.path.join(trajectories_folder, filename),\n",
    "                                      fitting_info=fitting_info, path_mapping=path_mapping,\n",
    "                                      starting_frame=sf,\n",
    "                                      dark_mode=dark_mode, dpi=dpi,\n",
    "                                      num_prev=num_prev, num_next=num_next,\n",
    "                                      num_frames=0)\n",
    "    plt.close(fig)\n",
    "    clear_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shot example (far team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sf in [1024, 1025]:\n",
    "    if sf<=1024:\n",
    "        num_prev = 5\n",
    "        num_next = 2\n",
    "    else:\n",
    "        num_prev = 6\n",
    "        num_next = 1\n",
    "    filename = f'{sf}.png'\n",
    "    fig, ax = create_trajectory_video(tracknet_v2, os.path.join(trajectories_folder, filename),\n",
    "                                      fitting_info=fitting_info, path_mapping=path_mapping,\n",
    "                                      starting_frame=sf,\n",
    "                                      dark_mode=dark_mode, dpi=dpi,\n",
    "                                      num_prev=num_prev, num_next=num_next,\n",
    "                                      num_frames=0)\n",
    "    plt.close(fig)\n",
    "    clear_output()\n",
    "\n",
    "for sf in [1538, 1547, 1549, 1555]:\n",
    "    if sf<=1547:\n",
    "        num_next = 4\n",
    "        if sf==1538:\n",
    "            num_prev = 2\n",
    "        else:\n",
    "            num_prev = 3\n",
    "    else:\n",
    "        num_prev = 4\n",
    "        num_next = 3\n",
    "    filename = f'{sf}.png'\n",
    "    fig, ax = create_trajectory_video(tracknet_v2, os.path.join(trajectories_folder, filename),\n",
    "                                      fitting_info=fitting_info, path_mapping=path_mapping,\n",
    "                                      starting_frame=sf,\n",
    "                                      dark_mode=dark_mode, dpi=dpi,\n",
    "                                      num_prev=num_prev, num_next=num_next,\n",
    "                                      num_frames=0)\n",
    "    plt.close(fig)\n",
    "    clear_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failure example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sf in [1495, 1496, 1499, 1501, 1502]:\n",
    "    filename = f'fail_{sf}.png'\n",
    "\n",
    "    display='k_min k_max params'\n",
    "    num_next=3\n",
    "    num_prev=2\n",
    "    if sf == 1495:\n",
    "        display='k_min k_max params'\n",
    "        display_prev = None\n",
    "        num_next=4\n",
    "        num_prev=1\n",
    "    else:\n",
    "        display_prev = 'k_max'\n",
    "\n",
    "    fig, ax = create_trajectory_video(tracknet_v2, os.path.join(trajectories_folder, filename),\n",
    "                                      fitting_info=fitting_info, path_mapping=path_mapping,\n",
    "                                      starting_frame=sf,\n",
    "                                      dark_mode=dark_mode, dpi=dpi,\n",
    "                                      num_next=num_next, num_prev=num_prev,\n",
    "                                      alpha_prev=1, alpha_next=1,\n",
    "                                      display=display, display_prev=display_prev, display_next='k_min',\n",
    "                                      show_outside_range=True,\n",
    "                                      num_frames=0)\n",
    "    clear_output()\n",
    "    # plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "695c89e8d1a0e3ecb3b35b5172c9b51550f30a782fca086e9907470f1e458eec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
